<!DOCTYPE html>
<html>

<head>
  <title>GLMs</title>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <link rel="stylesheet" href="fonts/quadon/quadon.css">
  <link rel="stylesheet" href="fonts/gentona/gentona.css">
  <link rel="stylesheet" href="slides_style_poldrack.css">
  <script type="text/javascript" src="assets/plotly/plotly-latest.min.js"></script>
</head>

<body>
  <textarea id="source">



<!-- TODO add slide numbers & maybe slide name -->

### A Crash Course on Generalized Linear Models (GLMs)

<centering>
  ![:scale 35%](images/stanford_s.png)
</centering>

Follow the slides: [ericwb.me/lectures/glms.html](ericwb.me/lectures/glms.html)

| Eric W. Bridgeford | {Psychology} |
| --- | --- |
| [ericwb95@gmail.com](mailto:ericwb95 at gmail dot com)  | [ericwb.me](https://ericwb.me) |

---
name:basics

### Outline

- Basics
- [Simulating and fitting](#simulate)
- [Evaluation and reporting](#evaluate)
- [Uncovered material](#uncovered)

---

### Notation table

| Notation | Interpretation |
| --- | --- |
| $x$ | scalar |
| $\\vec x$ | vector |
| $X$ | matrix |
| $\\mathbf x$ | a random variable (note bold face) |
| $\\vec{\\mathbf x}$ | a random vector, with elements $\\mathbf x_i$ |
| $\\mathbf X$ | a random matrix, with elements $\\mathbf x_{ij}$ |

---

#### Brief review: matrix multiplication

- Given a matrix $A$ that is $n \times m$ and a matrix $B$ that is $m \\times d$, the matrix product $AB$ is the matrix where $(AB)\_{ij} = \\sum\_{k = 1}^m a\_{ik}b\_{kj}$.
--

- Pictorally:

$$AB = \\begin{bmatrix}
{\\color{yellow}a\_{11}} & {\\color{yellow}...} & {\\color{yellow}a\_{1m}} \\\\
\\vdots & \\ddots & \\vdots \\\\
a\_{n1} & ... & a\_{nm}
\\end{bmatrix}\\begin{bmatrix}
{\\color{yellow}b\_{11}} & ... & b\_{1d} \\\\
{\\color{yellow}\\vdots} & \\ddots & \\vdots \\\\
{\\color{yellow}b\_{m1}} & ... & b\_{md}
\\end{bmatrix}$$

$$
\,\,\,\, = \\begin{bmatrix}
{\\color{yellow}\\sum\_{k = 1}^m a\_{1k}b\_{k1}} & ... & \\sum\_{k = 1}^m a\_{1k}b\_{kd} \\\\
\\vdots & \\ddots & \\vdots \\\\
\\sum\_{k = 1}^m a\_{nk}b\_{k1} & ... & \\sum\_{k = 1}^m a\_{nk}b\_{kd} 
\\end{bmatrix}
$$
--

- "Linear combination of the elements in a given row of $A$, with coefficients given by a given columns of $B$"

---

#### Application: matrix-vector multiplication

- Given a matrix $A$ that is $n \times d$ and a $d$-dimensional vector $\\vec \\eta$, then:

$$
A\\vec \\eta = \\begin{bmatrix}
  \\sum\_{k = 1}^d \\eta\_k a\_{1k} \\\\
  \\vdots \\\\
  \\sum\_{k = 1}^d \\eta\_k a\_{nk}
\\end{bmatrix}
$$

- "Linear combination of the elements in a given row of $A$, with coefficients given by $\\vec \\eta$"

---

#### Application: matrix-vector multiplication

- If the rows of $A$ are given by $\\vec a\_i$ for each row $i$, then:
$$
A\\vec \\eta = \\begin{bmatrix}
\\vdash & \\vec a\_{1}^\\top & \\dashv \\\\
& \\vdots & \\\\
\\vdash & \\vec a\_n^\\top & \\dashv
\\end{bmatrix} \\vec \\eta =
\\begin{bmatrix}
  \\vec a\_1^\\top \\vec \\eta \\\\
  \\vdots \\\\
  \\vec a\_n^\\top \\vec \\eta
\\end{bmatrix}
$$

- because $\\vec a\_i^\\top \\vec \\eta = \\sum\_{k = 1}^d \\eta\_k a\_{ik}$

---

#### Brief review: expected value

- If $\\mathbf y\_i$ is discrete (with possible values $\\mathcal Y$) and has a pmf $\\mathbb P()$, the expected value is the quantity:

$$\\mathbb E[\\mathbf y\_i] = \\sum\_{y \\in \\mathcal Y} y \\mathbb P(\\mathbf y\_i = y)$$

---

#### Brief review: expected value

- If $\\mathbf y\_i$ is discrete and has a pmf $\\mathbb P()$, the expected value is the quantity:

$$\\mathbb E[\\mathbf y\_i] = {\\color{yellow}\\sum\_{y \\in \\mathcal Y}} y \\mathbb P(\\mathbf y\_i = y)$$
- What do we want to compute the expected value over?

---

#### Brief review: expected value

- If $\\mathbf y\_i$ is discrete and has a pmf $\\mathbb P()$, the expected value is the quantity:

$$\\mathbb E[\\mathbf y\_i] = \\sum\_{y \\in \\mathcal Y} {\\color{yellow}y} \\mathbb P(\\mathbf y\_i = y)$$
- what is its value for this particular "increment"?

---

#### Brief review: expected value

- If $\\mathbf y\_i$ is discrete and has a pmf $\\mathbb P()$, the expected value is the quantity:

$$\\mathbb E[\\mathbf y\_i] = \\sum\_{y \\in \\mathcal Y} y {\\color{yellow}\\mathbb P(\\mathbf y\_i = y)}$$
- "How much" of the time is $\\mathbf y\_i$ the value $y$?

---

#### Brief review: expected value

- If $\\mathbf y\_i$ is discrete and has a pmf $\\mathbb P()$, the expected value is the quantity:

$$\\mathbb E[\\mathbf y\_i] = \\sum\_{y \\in \\mathcal Y} y \\mathbb P(\\mathbf y\_i = y)$$
- "the expected value is a weighted (by the pmf) average of the possible values that $\\mathbf y\_i$ could take"

---

#### Brief review: expected value

- If $\\mathbf y\_i$ is discrete and has a pmf $\\mathbb P()$, the expected value is the quantity:

$$\\mathbb E[\\mathbf y\_i] \\equiv \\mathbb E[\\mathbf y\_i; \\theta] = \\sum\_{y \\in \\mathcal Y} y \\mathbb P\_\\theta(\\mathbf y\_i = y)$$
- "the expected value is a weighted (by the pmf) average of the possible values that $\\mathbf y\_i$ could take"
- parameters of $\\mathbf y\_i$, given by $\\theta$, are typically$^\\dagger$ treated as constant with GLMs


$^\\dagger$: unless you're using Bayesian implementations.

---

#### Application: Bernoulli distribution

if $\\mathbf y\_i \sim Bernoulli(p)$, then the pmf is:
$$
\\mathbb P(\\mathbf y\_i = y\_i) = \\begin{cases}
  p, \,\,\,\, y\_i = 1 \\\\
  1 - p, \,\,\,\, y\_i = 0
\\end{cases} \\\\
 = p^{y\_i} (1 - p)^{1 - y\_i}
$$
--
So:
$$\\mathbb E[\\mathbf y\_i; p] = 1 \cdot p + 0 \cdot (1 - p) = p$$

---

### What does a GLM look like?

$g\\left(\\mathbb E\\left[{\\mathbf y}\\right]\\right) = X \\beta$



---

### .ye[Random] component

$g\\left(\\mathbb E\\left[{\\color{yellow}\\vec{\\mathbf y}}\\right]\\right) = X \\beta$

The "response" variables are the vector $\\vec{\\mathbf y}$, which is a vector:

$$
\\vec{\\mathbf y} = \\begin{bmatrix}
  \\mathbf y\_1 \\\\
  \\vdots \\\\
  \\mathbf y\_n
\\end{bmatrix}
$$

--

- $\\mathbf y_i$ are the .ye[responses] for each sample $i$ (e.g., heart disease or not)
--

- The responses are .ye[independent], and typically, modeled using the exponential family
  - includes Normal, Poisson, Bernoulli, Categorical, etc. response types

---

### .ye[Systematic] component

$g\\left(\\mathbb E\\left[\\vec{\\mathbf y}\\right]\\right) = {\\color{yellow}X \\beta}$

- For each sample $i$, $\\vec x_i$ is a $d(+1?)$-dimensional "feature vector" or set of "predictors" of the response
  - E.g., height, weight, biological sex, etc.
  - typically, $x_{i0} = 1$ is an "intercept" term
--

- organized in the "model" matrix $X$, which is the $n \\times d(+1?)$:

$$
X = \\begin{bmatrix}
  \\vdash & \\vec x_1^\\top & \\dashv \\\\
  & \\vdots & \\\\
  \\vdash & \\vec x_n^\\top & \\dashv
\\end{bmatrix}, \\,\\,\\,\\, X\\beta = \\begin{bmatrix}
\\vec x_1^\\top\\beta \\\\
\\vdots \\\\
\\vec x_n^\\top\\beta
\\end{bmatrix}
$$

--
- Note: elements of $X\\beta$ are .ye[linear combinations] of the predictors, as:
$\\vec x\_i^\\top \\beta = \\sum\_{j = 1(0?)}^d \\beta\_j x\_{ij}$

---

### .ye[Systematic] component

$g\\left(\\mathbb E\\left[\\vec{\\mathbf y}\\right]\\right) = {\\color{yellow}X \\beta}$

- For each sample $i$, $\\vec x_i$ is a $d(+1?)$-dimensional "feature vector" or set of "predictors" of the response
- organized in the "model" matrix $X$, which is the $n \\times d(+1?)$:

$$
X = \\begin{bmatrix}
  \\vdash & \\vec x_1^\\top & \\dashv \\\\
  & \\vdots & \\\\
  \\vdash & \\vec x_n^\\top & \\dashv
\\end{bmatrix}, \\,\\,\\,\\, X\\beta = \\begin{bmatrix}
\\vec x_1^\\top\\beta \\\\
\\vdots \\\\
\\vec x_n^\\top\\beta
\\end{bmatrix}
$$
- Note: elements of $X\\beta$ are linear combinations of the predictors, as:
$\\vec x\_i^\\top \\beta = \\sum\_{j = 1(0?)}^d \\beta\_j x\_{ij}$
- The predictors are treated as .ye[fixed]
---

### .ye[Link] function

${\\color{yellow}g\\big(}\\mathbb E\\left[\\vec{\\mathbf y}\\right]{\\color{yellow}\\big)} = X \\beta$

- The "link" relates the mean of the random component to the systematic component
  - typically monotonic and differentiable (allows us to estimate $\\beta$)
  - If the link $g(x) = x$ ("identity link"), standard "linear model"

--
- .ye[canonical link]: the link function that "makes the most sense" for a given distribution for the responses

---
name:simulate

### Outline

- [Basics](#basics)
- Simulating and fitting
- [Evaluation and reporting](#evaluate)
- [Uncovered material](#uncovered)

---

### Working example: logistic regression

$\\mathbf y_i \\sim Bernoulli(p\_i)$

Random component: $\\mathbb E[\\mathbf y\_i] = p\_i$, $Var(\\mathbf y\_i) = p\_i(1 - p\_i)$

--

Link function: $g(p\_i) = \\text{logit}(p\_i) = \\log \\frac{p\_i}{1 - p\_i}$
  - "log" of the "odds"
  - "odds" of $\\mathbf y\_i$: $\\frac{p_i}{1 - p_i}$
---
name:logit

### Properties of the logit link

$\\text{logit}(p\_i) = \\log \\frac{p\_i}{1 - p\_i} = \\vec x\_i^\\top \\beta$

$$\\frac{p\_i}{1 - p\_i} = \exp\\left(\\vec x\_i^\\top \\beta\\right)$$

--
$$ \\Rightarrow p\_i = (1 - p\_i) l\_i,\,\,\,\, l\_i = \exp\\left(\\vec x\_i^\\top \\beta\\right)$$

--
$$\\Rightarrow p\_i(1 + l\_i) = l\_i$$

--
$$\\Rightarrow p\_i = \\frac{\exp\\left(\\vec x\_i^\\top \\beta\\right)}{1 + \exp\\left(\\vec x\_i^\\top \\beta\\right)} = \text{expit}\\left(\\vec x\_i^\\top \\beta \\right)$$

---

### Simulations


[Collab link](https://colab.research.google.com/drive/1EePntLxO9jmMuDO8IoNSwfh6t_S9NC3g?usp=sharing), See Section 1

---
name:evaluate

### Outline

- [Basics](#basics)
- [Simulating and fitting](#simulate)
- Evaluation and reporting
- [Uncovered material](#uncovered)

---

### Estimation

- "Fitting" a regression model gives us estimates of the coefficients $\beta$, given by $\hat \beta$
  - "Fit": Approximate MLE (for GLMs, usually Fisher scoring, which with canonical link, is equivalent to Newton's method)

---

### Coefficient interpretation

Model: $\text{logit}(\\mathbb E[\\mathbf y\_i; x\_{i1}, x\_{i2}]) = \\beta\_0 + \\beta\_1 x\_{i1} + \\beta\_2 x\_{i2}$

Predictors: $x\_{i1}$ is binary, $x\_{i2}$ is continuous

Since $\\mathbf y\_i$ is binary, we can rewrite this:

$\text{logit}(\\mathbb P[\\mathbf y\_i = 1; x\_{i1}, x\_{i2}]) = \\beta\_0 + \\beta\_1 x\_{i1} + \\beta\_2 x\_{i2}$

---

### Coefficient interpretation (binary)

Model: $\text{logit}(\\mathbb P[\\mathbf y\_i = 1; x\_{i1}, x\_{i2}]) = \\beta\_0 + \\beta\_1 x\_{i1} + \\beta\_2 x\_{i2}$

Step 1: Isolate the coefficient

$\text{logit}(\\mathbb P[\\mathbf y\_i = 1; x\_{i1} = 1, x\_{i2}]) - \text{logit}(\\mathbb P[\\mathbf y\_i = 1; x\_{i1} = 0, x\_{i2}])$

--

$\,\,\,\, = \left(\\beta\_0 + \\beta\_1 \cdot 1 + \\beta\_2 x\_{i2}\right) - \\left(\\beta\_0 + \\beta\_1 \cdot 0 + \\beta\_2 x\_{i2}\\right)$

--

$\,\,\,\, = \\beta\_1$

---

### Coefficient interpretation (binary)

Step 2: make it something meaningful

$\\beta\_1 = \text{logit}(\\mathbb P[\\mathbf y\_i = 1; x\_{i1} = 1, x\_{i2}]) - \text{logit}(\\mathbb P[\\mathbf y\_i = 1; x\_{i1} = 0, x\_{i2}])$

"Difference in log odds?"

--

With $p\_{i, x} = \\mathbb P[\\mathbf y\_i = 1; x\_{i1} = x, x\_{i2}]$:
$$\\beta\_1 = \\log\\frac{p\_{i, 1}}{1 - p\_{i, 1}} - \\log\\frac{p\_{i, 0}}{1 - p\_{i, 0}} = \\log\\frac{\\frac{p\_{i, 1}}{1 - p\_{i, 1}}}{\\frac{p\_{i, 0}}{1 - p\_{i, 0}}}$$

--

$$\\exp(\\beta\_1) = \\frac{\\frac{p\_{i, 1}}{1 - p\_{i, 1}}}{\\frac{p\_{i, 0}}{1 - p\_{i, 0}}}$$

---

### Coefficient interpretation (binary)

$$\\exp(\\beta\_1) = \\frac{\\frac{p\_{i, 1}}{1 - p\_{i, 1}}}{\\frac{p\_{i, 0}}{1 - p\_{i, 0}}}$$

--
- "odds ratio"  

- $\\exp(\\hat{\\beta}\_1)$ gives a multiplicative change in the odds for the response .ye[with] the predictor ($x\_{i1} = 1$) as-compared to .ye[without] ($x\_{i1} = 0$)

---

### Coefficient interpretation (continuous)

$$\\beta\_2 = \text{logit}(\\mathbb P[\\mathbf y\_i = 1; x\_{i1}, x\_{i2} = x+1]) - $$
$$\,\,\,\,\,\,\,\,\,\text{logit}(\\mathbb P[\\mathbf y\_i = 1; x\_{i1}, x\_{i2} = x])$$

Similar derivation to before... with $q\_{i,y} = \\mathbb P[\\mathbf y\_i = 1; x\_{i1}, x\_{i2} = y]$

$$\\exp(\\beta\_2) = \\frac{\\frac{q\_{i, x+1}}{1 - q\_{i, x+1}}}{\\frac{q\_{i, x}}{1 - q\_{i, x}}}$$

- For a $1$-unit change in the predictor $x\_{i2}$, the odds for the response changes by a multiplicative factor of $\\exp(\\hat\\beta\_2)$

---
name:asy

### "Asymptotics"

- Under appropriate regularity conditions (e.g., assumptions inherent to a GLM):
$$\\hat{\\pmb\\beta}_j  - \\beta\_j\\xrightarrow[n \\rightarrow \\infty]{\\mathcal D} \\mathcal N\\left(0, \\sigma^2\\right)$$

- Note: $\\hat{\\pmb\\beta}\_j$ is the "estimator" (random... "estimates" are based on the data)
  - $\\sigma$: "variance" of the estimator
  - Estimated with standard error $\\hat{\\pmb \\sigma} \\xrightarrow[n \\rightarrow \\infty]{\\mathcal P} \\sigma$
---

### Testing

- "Are the coefficients impactful?"
  - Do they actually change the log-odds for the response?
--

- Recall: interpretations are of the form...
$$\\exp(\\beta\_j) = \text{odds ratio}$$
--

- If the coefficient is $0$, then $\text{odds ratio} = 1$ (no change in response probability)
--

- "Null hypothesis", typically "no effect": $H_0 : \\beta\_j = 0$
- "Alternative hypothesis", typically "some effect": $H_A: \\beta\_j \\neq 0$
  - "two-tailed": both extreme (negative) or extreme (positive) are "noteworthy"
---

### "Wald" Test

- Under $H_0: \\beta\_j = 0$, Equation on [Asymptotics](#asy) gives:
$$\\hat{\\pmb\\beta}_j\\xrightarrow[n \\rightarrow \\infty]{\\mathcal D} \\mathcal N\\left(0, \\sigma^2\\right)$$
- "How extreme is an estimate $\\hat \\beta_j$, relative $\\mathcal N(0, \\sigma^2)$?"
--

- Equivalent: $\\frac{\\hat{\\pmb\\beta}_j}{\\hat{\\pmb\\sigma}} \\xrightarrow[n \\rightarrow \\infty]{\\mathcal D} \\mathcal N\\left(0, 1\\right)$
- Test statistic: $\\mathbf z = \\frac{\\hat{\\pmb\\beta}_j}{\\hat{\\pmb\\sigma}}$
- Assumption: $\\mathbf z \\overset{approx}{\\sim} \\mathcal N(0, 1)$
--

- Compare $z$ to $\\mathcal N(0, 1)$... "Wald" test or "Z" test

---

### "Confidence" Intervals

- Equation on [Asymptotics](#asy) can be rearranged:
$$\\frac{\\hat{\\pmb\\beta}_j - \\beta_j}{\\hat{\\pmb\\sigma}} \\xrightarrow[n \\rightarrow \\infty]{\\mathcal D} \\mathcal N\\left(0, 1\\right)$$

People like to write:

$$\\mathbb P\\left(\\frac{\\hat{\\pmb\\beta}\_j - \\beta\_j}{\\hat{\\pmb\\sigma}} \\in [z\_{\\alpha/2}, z\_{1 - \\alpha/2}]\\right) = 1 - \\alpha$$

--
And then with rearrangement and symmetry arguments of normal distribution...

$$\\mathbb P\\left(\\beta\_j \\in \\left[\\hat{\\pmb \\beta}\_j - \\hat{\\pmb\\sigma} z\_{\\alpha/2}, \\hat{\\pmb \\beta}\_j  + \\hat{\\pmb\\sigma} z\_{\\alpha/2}\\right]\\right) = 1 - \\alpha$$

---

### "Confidence" Intervals

$$\\mathbb P\\left(\\beta\_j \\in \\left[\\hat{\\pmb \\beta}\_j - \\hat{\\pmb\\sigma} z\_{\\alpha/2}, \\hat{\\pmb \\beta}\_j  + \\hat{\\pmb\\sigma} z\_{\\alpha/2}\\right]\\right) = 1 - \\alpha$$
--

- Note: the .ye[interval] is what is random

--
- When we perform the experiment, we "realize" a single $\\hat{\\beta}\_j$ and $\\hat \\sigma$ (our estimates)
  - use this and properties of the estimator to construct one such interval

--
- This interval has two possibilities, since $\\beta\_j$ and $\\hat \\sigma$ are fixed (called "coverage"):
  - $\\beta\_j \\in \\left[\\hat{\\beta}\_j - \\hat\\sigma z\_{\\alpha/2}, \\hat{\\beta}\_j  + \\hat\\sigma z\_{\\alpha/2}\\right]$ (parameter covered), or
  - $\\beta\_j \\not\\in \\left[\\hat{\\beta}\_j - \\hat\\sigma z\_{\\alpha/2}, \\hat{\\beta}\_j  + \\hat\\sigma z\_{\\alpha/2}\\right]$ (parameter not covered)

---

### "Confidence" Intervals

$$\\mathbb P\\left(\\beta\_j \\in \\left[\\hat{\\pmb \\beta}\_j - \\hat{\\pmb \\sigma} z\_{\\alpha/2}, \\hat{\\pmb \\beta}\_j  + \\hat{\\pmb \\sigma} z\_{\\alpha/2}\\right]\\right) = 1 - \\alpha$$

- The probability statement refers to the probability of coverage of these random intervals repeated over many experiments identical to the one analyzed
  - any one realized interval either covers, or does not (no probabilistic interpretation)
  - "If the experiment were repeated many times, $1 - \\alpha$ of the $1 - \\alpha$ CIs will tend to cover the true population parameter"
--

- My recommendation: report it, because you have to, and avoid trying to make sense of it or interpreting it

[Collab link](https://colab.research.google.com/drive/1EePntLxO9jmMuDO8IoNSwfh6t_S9NC3g?usp=sharing), See Section 2

---

### The likelihood

- We can use properties of a GLM to construct a likelihood. For instance, for a logistic regression model:

$$\\mathcal L(\\beta; \\vec y, X) = \\prod\_{i = 1}^n \\mathbb P(\\mathbf y\_i = y\_i; \\vec \\beta, X)$$
$$\,\,\,\, = \\prod\_{i = 1}^n p\_i^{y\_i}(1 - p\_i)^{1 - y\_i}$$
$$\,\,\,\, = \\prod\_{i = 1}^n \\text{expit}(X\\beta)^{y\_i}(1 - \\text{expit}(X\\beta))^{1 - y\_i}$$
... from [properties of the logit](#logit)

---

### Maximum Likelihood Estimation

The MLE $\\hat \\beta$ that we solved for is, in the ideal case:
$$\\hat \\beta = \\argmax\_{\\beta \\in \\Theta}\\mathcal L(\\beta; \\vec y, X)$$

- $\\Theta$: "what sorts of models are allowed?"
--

- Easiest case: models with all of the possible covariates included
- In our example, $\\Theta$ defined that we "allowed" models with $\\beta\_0$ (intercept), $\\beta\_1$ (for $x\_{i1}$), $\\beta\_2$ (for $x\_{i2}$), $\\beta\_3$ (for $x\_{i3}$)

---

### Likelihood Ratio Statistic

$$\\Lambda = \\frac{\\max\_{\\vec\\beta \\in \\Theta}\\mathcal L(\\vec\\beta; \\vec y, X)}{\\max\_{\\vec\\beta \\in \\Theta\_s}\\mathcal L(\\vec\\beta; \\vec y, X)}$$

- where $\\Theta\_s \\subseteq \\Theta$
- Ratio of likelihood for the best possible (more complex) model to the best possible (less complex) model
  - Example: $\\Theta$ defined that we "allowed" models with $\\beta\_0$ (intercept), $\\beta\_1$ (for $x\_{i1}$), $\\beta\_2$ (for $x\_{i2}$), $\\beta\_3$ (for $x\_{i3}$)
  - What if $\\Theta\_s$ .ye[omits] the term for $\\beta\_3$?
--

- Relevant: degrees of freedom for a given model is the number of parameters
  - E.g., $\\Theta$ has $4$ terms, $\\Theta\_s$ has $3$ terms

---

### Wilks' Theorem

Under some regularity conditions, with:

$$\\Lambda = \\frac{\\max\_{\\vec\\beta \\in \\Theta}\\mathcal L(\\vec\\beta; \\vec y, X)}{\\max\_{\\vec\\beta \\in \\Theta\_s}\\mathcal L(\\vec\\beta; \\vec y, X)}$$

where $\\Theta$ is a model that allows $p$ terms and $\\Theta\_s$ is a model that allows $p\_s$ terms:
$$2\\log\\Lambda \\xrightarrow[n \\rightarrow \\infty]{\\mathcal D} \\chi^2\_{p - p\_s}$$

If the true parameter $\\vec\\beta \\in \\Theta\_s$

- "If the bigger model is better, the likelihood ratio statistic will be extreme"
--

- Allows us to test $H\_0: \\vec\\beta \\in \\Theta\_s$ against $H\_A: \\beta \\in \\Theta \\setminus \\Theta\_s$

---

### Likelihood ratio test (LRT)

$$2\\log \\Lambda = -2 \\left(\\max\_{\\vec\\beta \\in \\Theta}\\ell(\\vec\\beta; \\vec y, X) - \\max\_{\\vec\\beta \\in \\Theta\_s}\\mathcal \\ell(\\vec\\beta; \\vec y, X)\\right)$$

- Using this as the test statistic, compare to $\\chi^2\_{p - p\_s}$ distribution to obtain a $p$-value

--
- Bias/variance tradeoff:
  - a fit model with .ye[every] possible parameter has the most possibilities, but also the highest possibility of overfitting
  - The LRT is often used as a surrogate to find models that fit the data "approximately" as well as more complicated ones

--
- the goal is model parsimony (simple but explanatory), and the LRT is one way we can check that

[Collab link](https://colab.research.google.com/drive/1EePntLxO9jmMuDO8IoNSwfh6t_S9NC3g?usp=sharing), See Section 3

---

name:uncovered

### Outline

- [Basics](#basics)
- [Simulating and fitting](#simulate)
- [Evaluation and reporting](#evaluate)
- Uncovered material

---

### How do you check whether your model is useful (at all)?

- Most GLMs: Likelihood ratio test, where "smaller" model includes no covariates
  - "Bernoulli" logistic regression models: nuance...
--

- There are weird behaviors comparing bernoulli models to intercept-only models
  - best practice: ROC curves (check sensitivity/specificity)

---

### Other stuff?

- Forwards/backwards model selection?
- Wald tests: they definitely provide numbers
  - better: "robust" tests (robust Z test, bootstrapping)
  - "robust" allow flexibility for .ye[some] of the assumptions made
- multiple comparisons problems: we produced a lot of $p$-values...
  - FWER control?
- incorporating non-linearities into linear models?
- other types of GLMs?
- @Jeanette?

</textarea>
<!-- <script src="https://gnab.github.io/remark/downloads/remark-latest.min.js"></script> -->
<!-- <script src="remark-latest.min.js"></script> -->
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.3/katex.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.3/contrib/auto-render.min.js"></script>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.3/katex.min.css">
<script type="text/javascript">

  var options = {};
  var renderMath = function () {
    renderMathInElement(document.body);
    // or if you want to use $...$ for math,
    renderMathInElement(document.body, {
      delimiters: [ // mind the order of delimiters(!?)
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false },
        { left: "\\[", right: "\\]", display: true },
        { left: "\\(", right: "\\)", display: false },
      ]
    });
  }

  remark.macros.scale = function (percentage) {
    var url = this;
    return '<img src="' + url + '" style="width: ' + percentage + '" />';
  };

  // var slideshow = remark.create({
  // Set the slideshow display ratio
  // Default: '4:3'
  // Alternatives: '16:9', ...
  // {
  // ratio: '16:9',
  // });

  var slideshow = remark.create(options, renderMath);


</script>
</body>

</html>